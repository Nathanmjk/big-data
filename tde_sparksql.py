# -*- coding: utf-8 -*-
"""TDE - SparkSQL

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SmwSqm02kpjfAkIQqQHBRIvwIKC-qZFu

#Alunos: Nathan Machado Josviak e Carlos Eduardo Rodrigues
"""

!pip install pyspark

import pyspark
from pyspark.sql import SparkSession

spark = SparkSession\
            .builder\
            .master('local[*]')\
            .appName('TDE')\
            .getOrCreate()
sc = spark.sparkContext

!wget https://jpbarddal.github.io/assets/data/bigdata/transactions_amostra.csv.zip
!unzip transactions_amostra.csv.zip
from pyspark.sql.functions import split, col, max, min, avg, count

df = spark.read.csv('transactions_amostra.csv', sep=';', header=True, inferSchema=True)
df.createOrReplaceTempView('tabela')



df.show(5, truncate=False)

# Exercicio 1 - The number of transactions involving Brazil
# utilizando query SQL
query = '''
       SELECT COUNT(*)
       FROM tabela
       WHERE country_or_area = 'Brazil'
      '''
spark.sql(query).show(1)

# Exercicio 1 com operações e tranformações
df.where(col('country_or_area') == 'Brazil').count()

# Exercicio 1 EM PANDAS
tbl = df.to_pandas_on_spark()
len(tbl[(tbl['country_or_area'] == 'Brazil')])

# Exercicio 2 - The number of transactions per flow type and year
# utilizando query SQL
query = '''
        SELECT flow, year, COUNT(*)
        FROM tabela
        GROUP BY flow, year
        '''
spark.sql(query).show(15)

# Exercicio 2 com operações e tranformações
transactions = df.groupBy(col("flow"), col("year")).count()
transactions.show(15)

# Exercicio 3 - The average of commodity values per year;
# utilizando query SQL
query = '''
        SELECT year, avg(trade_usd)
        from tabela
        GROUP BY year

        '''

spark.sql(query).show(15)

# # Exercicio 4 - The average price of commodities per unit type, year, and category in the export flow
# Utilizando uma query em sql
query = '''
        SELECT quantity_name, year, category, AVG(trade_usd) AS PrecoMedio
        FROM tabela
        WHERE country_or_area = 'Brazil' AND flow = 'Export'
        GROUP BY quantity_name, year, category
        '''
spark.sql(query).show(15)

# # Exercicio 4 com operações e tranformações

# filtrando as colunas por brazil e Export
ExportBR = df.filter((df["country_or_area"] == "Brazil") & (df["flow"] == "Export"))

# Agrupando as transações por tipo, ano e categoria e fazendo o preço médio das commodities
avg = ExportBR.groupBy("quantity_name", "year", "category").agg(avg("trade_usd").alias("AVG"))


avg.show(10)

# Exercicio 5 - The maximum, minimum, and mean transaction price per unit type and year
# Utilizando uma query em sql

query = '''
        SELECT quantity_name, year, MAX(CAST(trade_usd AS DOUBLE)) AS MaxPrice, MIN(CAST(trade_usd AS DOUBLE)) AS MinPrice, AVG(CAST(trade_usd AS DOUBLE)) AS MeanPrice
        FROM tabela
        GROUP BY quantity_name, year
        '''
spark.sql(query).show(12)

# Exercicio 6 - The country with the largest average commodity price in the Export flow;
# Utilizando uma query em sql
query = '''
        SELECT country_or_area, AVG(trade_usd) AS PrecoMedio
        FROM tabela
        WHERE flow = 'Export'
        GROUP BY country_or_area
        ORDER BY PrecoMedio DESC
        LIMIT 1
        '''

spark.sql(query).show(10)

# Exercicio 7 - The most commercialized commodity (summing the quantities) in 2016, per flow type.
# Utilizando uma query em sql
query = '''
        SELECT flow, commodity, SUM(quantity) AS total_quantity
        FROM tabela
        WHERE year = 2016
        GROUP BY flow, commodity
        ORDER BY total_quantity DESC
        LIMIT 1
        '''

spark.sql(query).show()